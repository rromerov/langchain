{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started tutorial with Llama3 using Langchain\n",
    "First we will install the necessary packages and then we will use the Llama3 model to generate some responses from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.18)\n",
      "Requirement already satisfied: langchain-text-splitters in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: langchain-postgres in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.13)\n",
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-0.2.3-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (0.3.40)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (2.0.38)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (3.11.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.4 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-community) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: pgvector<0.4 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-postgres) (0.3.6)\n",
      "Requirement already satisfied: psycopg<4,>=3 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-postgres) (3.2.5)\n",
      "Requirement already satisfied: psycopg-pool<4.0.0,>=3.2.1 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-postgres) (3.2.6)\n",
      "Collecting ollama<1,>=0.4.4 (from langchain-ollama)\n",
      "  Downloading ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from psycopg<4,>=3->langchain-postgres) (2022.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\omen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.1.0)\n",
      "Downloading langchain_ollama-0.2.3-py3-none-any.whl (19 kB)\n",
      "Downloading ollama-0.4.7-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: ollama, langchain-ollama\n",
      "Successfully installed langchain-ollama-0.2.3 ollama-0.4.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain-text-splitters langchain-postgres langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, we will use the Llama3 model to generate some responses from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"...blue! (or at least, that's what I'm assuming, depending on the time of day and location!) Is there something specific you'd like to know about the sky?\", additional_kwargs={}, response_metadata={'model': 'llama3', 'created_at': '2025-02-28T21:00:14.5416772Z', 'done': True, 'done_reason': 'stop', 'total_duration': 8877243200, 'load_duration': 2743888000, 'prompt_eval_count': 14, 'prompt_eval_duration': 915000000, 'eval_count': 38, 'eval_duration': 5214000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-6fb050ad-62b8-4990-9558-f1017bbdc110-0', usage_metadata={'input_tokens': 14, 'output_tokens': 38, 'total_tokens': 52})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOllama(model='llama3')\n",
    "model.invoke(\"The sky is:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **invoke** method is used to interact with the model. The **invoke** method takes a dictionary as input and returns a dictionary as output. The input dictionary should contain the input text and a set of parameters including **temperature**. Setting temperature to values such as 0.9 leads to more creative responses, lower values like 0.1 generate more predictable outputs. The output dictionary will contain the response generated by the model. \n",
    "\n",
    "Generative AI models have parameters such as **max_tokens** that limits the size (and cost) of the output, a low value can cause the output generation to stop prematurely, so it may appear truncated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using roles with Langchain\n",
    "In order to use them, chat model's interface makes it easier to configure and manage conversions in your AI chatbot application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3', 'created_at': '2025-02-28T21:25:40.5015663Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5328784200, 'load_duration': 2998972700, 'prompt_eval_count': 17, 'prompt_eval_duration': 1299000000, 'eval_count': 8, 'eval_duration': 1029000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-bf64e52c-3ba4-450d-a512-69a18fd6aacf-0', usage_metadata={'input_tokens': 17, 'output_tokens': 8, 'total_tokens': 25})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt = [HumanMessage(\"What is the capital of France?\")]\n",
    "\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat models use different types of chat message interfaces associated with roles mentioned earlier, that include the following:\n",
    "- **HumanMessage**: Message sent from the perspective of the human (*user* role)\n",
    "- **AIMessage**: Message sent from the perspective of the AI that the human is interacting with (*assistant* role)\n",
    "- **SystemMessage**: Message setting the instructions the AI should follow (*system* role)\n",
    "- **ChatMessage**: A message allowing for arbitrary setting of role.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Paris!!!', additional_kwargs={}, response_metadata={'model': 'llama3', 'created_at': '2025-02-28T22:38:15.8031557Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4579676600, 'load_duration': 2735132700, 'prompt_eval_count': 37, 'prompt_eval_duration': 1010000000, 'eval_count': 3, 'eval_duration': 257000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-6625fcbe-4e70-491c-b8d6-7d0795ab8f0d-0', usage_metadata={'input_tokens': 37, 'output_tokens': 3, 'total_tokens': 40})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"llama3\")\n",
    "system_msg = SystemMessage(\n",
    "    '''You are a helpful assistant that responds to questions with three exclamation marks.'''\n",
    ")\n",
    "\n",
    "human_msg = HumanMessage('What is the capital of France?')\n",
    "\n",
    "model.invoke([system_msg,human_msg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusable prompts\n",
    "Promps help generate better responses as it allows the model to understand the context and generate relevant answers to queries.\n",
    "\n",
    "Here is a detailed prompt: \n",
    "\n",
    "```\n",
    "Answer the question based on the context below. If the question cannot be\n",
    "answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: The most recent advancements in NLP are being driven by Large Language \n",
    "Models (LLMs). These models outperform their smaller counterparts and have\n",
    "become invaluable for developers who are creating applications with NLP \n",
    "capabilities. Developers can tap into these models through Hugging Face's\n",
    "`transformers` library, or by utilizing OpenAI and Cohere's offerings through\n",
    "the `openai` and `cohere` libraries, respectively.\n",
    "\n",
    "Question: Which model providers offer LLMs?\n",
    "\n",
    "Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge here is to figure out what the text should contain and how it should vary based on the user's input. In the provided example, context and question are hardcoded, but what if we want to pass these in dynamically?\n",
    "\n",
    "LangChain provides **prompt template interfaces** that make it easy to construct promps with dynamic inputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Answer the question based on the context below. If the question cannot be answered\\n                                        using the information provided, answer with \"I don\\'t know\".\\n                                        Context: The most recent advancements in NLP are being driven by Large \\n        Language Models (LLMs). These models outperform their smaller \\n        counterparts and have become invaluable for developers who are creating \\n        applications with NLP capabilities. Developers can tap into these \\n        models through Hugging Face\\'s `transformers` library, or by utilizing \\n        OpenAI and Cohere\\'s offerings through the `openai` and `cohere` \\n        libraries, respectively.\\n                                        \\n                                        Question: Which model providers offer LLMs?\\n                                        \\n                                        Answer: ')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered\n",
    "                                        using the information provided, answer with \"I don't know\".\n",
    "                                        Context: {context}\n",
    "                                        \n",
    "                                        Question: {question}\n",
    "                                        \n",
    "                                        Answer: \"\"\")\n",
    "\n",
    "template.invoke({\n",
    "    \"context\": \"\"\"The most recent advancements in NLP are being driven by Large \n",
    "        Language Models (LLMs). These models outperform their smaller \n",
    "        counterparts and have become invaluable for developers who are creating \n",
    "        applications with NLP capabilities. Developers can tap into these \n",
    "        models through Hugging Face's `transformers` library, or by utilizing \n",
    "        OpenAI and Cohere's offerings through the `openai` and `cohere` \n",
    "        libraries, respectively.\"\"\",\n",
    "        \"question\": \"Which model providers offer LLMs?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This make the static prompt into a dynamics one, the **template** object contains the structure of the final prompt, alongside with the dynamic inputs that will be inserted. The **invoke** method dynamically replaces the placeholders with the actual values using f-string syntax. Let's see a full example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\" Answer the question based on the context below. If the question cannot be answered using \n",
    "                          the information providedm answer with \"I don't know\".\n",
    "\n",
    "                                        Context: {context}\n",
    "                                        \n",
    "                                        Question: {question}\n",
    "                          \n",
    "                          Answer: \"\"\")\n",
    "\n",
    "model = ChatOllama(model=\"llama3\")\n",
    "\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"\"\"The most recent advancements in NLP are being driven by Large\n",
    "        Language Models (LLMs). These models outperform their smaller \n",
    "        counterparts and have become invaluable for developers who are creating \n",
    "        applications with NLP capabilities. Developers can tap into these \n",
    "        models through Hugging Face's `transformers` library, or by utilizing \n",
    "        OpenAI and Cohere's offerings through the `openai` and `cohere` \n",
    "        libraries, respectively.\"\"\",\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})\n",
    "\n",
    "completion = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, OpenAI and Cohere offer Large Language Models (LLMs). Therefore, the answer is:\n",
      "\n",
      "OpenAI and Cohere.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(completion.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: For AI chat applications you can use **ChatPromptTemplate** to provide dynamic inptus based on the role of the chat message.\n",
    "\n",
    "A basic example could be as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Answer the question based on the context below. If the question cannot be\\nanswered using the information provided, answer with \"I don\\'t know\".', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Context: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Question: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\", additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Answer the question based on the context below. If the question cannot be\n",
    "answered using the information provided, answer with \"I don't know\".\"\"\"),\n",
    "    (\"human\",\"Context: {context}\"),\n",
    "    (\"human\",\"Question: {context}\")\n",
    "])\n",
    "\n",
    "template.invoke({\n",
    "    \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\"\"\",\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look deeper the **ChatPromptTemplass** object uses one **SystemMessage**and two **HumanMessage** that contain the context and the question dynamically. You can still format the template in the same way and pass it to a large language model for prediction output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='According to the context, Hugging Face (through their `transformers` library), OpenAI (through their `openai` library), and Cohere (through their `cohere` library) are the model providers that offer Large Language Models (LLMs).', additional_kwargs={}, response_metadata={'model': 'llama3', 'created_at': '2025-03-02T20:16:15.800797Z', 'done': True, 'done_reason': 'stop', 'total_duration': 11097851100, 'load_duration': 31124100, 'prompt_eval_count': 152, 'prompt_eval_duration': 369000000, 'eval_count': 54, 'eval_duration': 10695000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-1e3ec8ad-313f-47d2-855d-4f53e194cde7-0', usage_metadata={'input_tokens': 152, 'output_tokens': 54, 'total_tokens': 206})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system','''Answer the question based on the context below. If the question cannot be answered using the information\n",
    "     provided, answer with \"I don't know\".'''),\n",
    "     ('human','Context: {context}'),\n",
    "     ('human','Question: {question}'),\n",
    "])\n",
    "\n",
    "model = ChatOllama(model='llama3',num_predict=100)\n",
    "\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"\"\"\"The most recent advancements in NLP are being driven by \n",
    "        Large Language Models (LLMs). These models outperform their smaller \n",
    "        counterparts and have become invaluable for developers who are creating \n",
    "        applications with NLP capabilities. Developers can tap into these \n",
    "        models through Hugging Face's `transformers` library, or by utilizing \n",
    "        OpenAI and Cohere's offerings through the `openai` and `cohere` \n",
    "        libraries, respectively.\"\"\",\n",
    "        \"question\":\"Which model providers offer LLMs?\"\n",
    "})\n",
    "\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured output using a LLM\n",
    "Plain text can be useful, however there are certain cases where is necessary to have a **structured output**. For example, when you want to generate a JSON, XML, CSV or any programming language such as Python, Java, etc.\n",
    "\n",
    "## JSON output\n",
    "This is the most commmon structured output to generate with LLMs. The output could be used for your fronted or saved in a database. When generating JSON, the first thing to do is define the schema that you want the LLM to respect when producing the output. After that, you should include it in your prompt, along with the text you want to use as a source. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "registry.ollama.ai/library/llama3:latest does not support tools (status code: 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOllama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, num_predict\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m     12\u001b[0m structured_llm \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mwith_structured_output(AnswerWithJustification)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mstructured_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[38;5;124;43mWhat weighs more, a pound of bricks or a pound of feathers\u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OMEN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   3021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3024\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\OMEN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py:5360\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5355\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5356\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5357\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5358\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5359\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   5361\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5362\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5363\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5364\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\OMEN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    283\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 284\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    285\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    286\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    287\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    288\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    289\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    290\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    291\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    292\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    293\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\OMEN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:860\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    854\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    859\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OMEN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:690\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 690\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    691\u001b[0m                 m,\n\u001b[0;32m    692\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    693\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    694\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    695\u001b[0m             )\n\u001b[0;32m    696\u001b[0m         )\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\OMEN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:925\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 925\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    926\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    927\u001b[0m         )\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    929\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OMEN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_ollama\\chat_models.py:701\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    696\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    700\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m--> 701\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_stream_with_aggregation(\n\u001b[0;32m    702\u001b[0m         messages, stop, run_manager, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    703\u001b[0m     )\n\u001b[0;32m    704\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[0;32m    705\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[0;32m    706\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[0;32m    707\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    711\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[0;32m    712\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\OMEN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_ollama\\chat_models.py:602\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[1;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    595\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    600\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[0;32m    601\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    604\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m ChatGenerationChunk(\n\u001b[0;32m    605\u001b[0m                 message\u001b[38;5;241m=\u001b[39mAIMessageChunk(\n\u001b[0;32m    606\u001b[0m                     content\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    619\u001b[0m                 ),\n\u001b[0;32m    620\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\OMEN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_ollama\\chat_models.py:589\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[1;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[0;32m    586\u001b[0m chat_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_params(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 589\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params)\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params)\n",
      "File \u001b[1;32mc:\\Users\\OMEN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ollama\\_client.py:168\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    167\u001b[0m   e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 168\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[0;32m    171\u001b[0m   part \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "\u001b[1;31mResponseError\u001b[0m: registry.ollama.ai/library/llama3:latest does not support tools (status code: 400)"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user's question along with justification for the answer.'''\n",
    "    answer : str\n",
    "    '''The answer to the user's question'''\n",
    "    justification : str\n",
    "    '''Justification for the answer'''\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\", temperature=0, num_predict=256)\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "structured_llm.invoke('''What weighs more, a pound of bricks or a pound of feathers''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that llama3 is not able to use tools, so we will have to use instead **llama3.1**\n",
    "\n",
    "For that we can use\n",
    "\n",
    "```cmd\n",
    "ollama pull llama3.1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerWithJustification(answer='a pound of bricks', justification='because the weight of an object is determined by its mass, and since both objects weigh one pound, they are equal in weight')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user's question along with justification for the answer.'''\n",
    "    answer : str\n",
    "    '''The answer to the user's question'''\n",
    "    justification : str\n",
    "    '''Justification for the answer'''\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0, num_predict=256)\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "structured_llm.invoke('''What weighs more, a pound of bricks or a pound of feathers''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see two things, first one that the LLM model made a mistake in the answer, secondly a schema was defined in the **BaseModel** pydanctic class. The method **with_structured_output** will use the schema to convert it to a **JSONSchema** object, which will be sent to the LLM model. Depending on the LLM, Langchain picks the best method to do this transformation, usually by **function calling** or **prompting**. Aditionally, the schema will be used to validate the output of the LLM model before returning it to the user, this ensures the output produced respects the schema defined previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Other formats using Output Parsers**\n",
    "As discussed earlier, other formats than can be used include XMLs and CSVs. This is where **output parsers** are handy. These are classes that assist you for structuring large language model respones. They have 2 main functions:\n",
    "\n",
    "1. **Providing format instructions:** Output parsers can be used to inject additional instructions in the prompt that will help guide the LLM to output text in the format it knows how to parse.\n",
    "2. **Validating and parsing the output:** The main fuction is to take the textual output of the LLM or chat model and render it to a more structured format, such as list, XML, or other format. This include removing extraneous information, correcting incomplete output, and validating the parsed values.\n",
    "\n",
    "To implement it on Python you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'cherry']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "items = parser.invoke('apple, banana, cherry')\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful methods, also known as runnable interfaces.\n",
    "- **invoke**: Transforms a single input to an input.\n",
    "- **batch**: efficiently transfroms multiple inputs into multiple outputs.\n",
    "- **stream**: Streams output from a single input as it's produced, useful for long running tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion: It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "Completions: [AIMessage(content=\"It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-03-02T22:47:09.1697228Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3715163700, 'load_duration': 35398900, 'prompt_eval_count': 13, 'prompt_eval_duration': 147000000, 'eval_count': 23, 'eval_duration': 3531000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-0ed2b26e-d26e-4464-9379-d8f93a63c0fd-0', usage_metadata={'input_tokens': 13, 'output_tokens': 23, 'total_tokens': 36}), AIMessage(content='It was nice chatting with you. If you ever want to talk again, feel free to come back anytime. Have a great day! Bye!', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-03-02T22:47:14.3026138Z', 'done': True, 'done_reason': 'stop', 'total_duration': 8846825200, 'load_duration': 34169400, 'prompt_eval_count': 13, 'prompt_eval_duration': 608000000, 'eval_count': 30, 'eval_duration': 4523000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-f052bcb4-8982-4c9a-b5ec-96916b927c94-0', usage_metadata={'input_tokens': 13, 'output_tokens': 30, 'total_tokens': 43})]\n",
      "Index 1: It\n",
      "Index 2:  was\n",
      "Index 3:  nice\n",
      "Index 4:  chatting\n",
      "Index 5:  with\n",
      "Index 6:  you\n",
      "Index 7: .\n",
      "Index 8:  If\n",
      "Index 9:  you\n",
      "Index 10:  ever\n",
      "Index 11:  want\n",
      "Index 12:  to\n",
      "Index 13:  talk\n",
      "Index 14:  or\n",
      "Index 15:  need\n",
      "Index 16:  anything\n",
      "Index 17: ,\n",
      "Index 18:  feel\n",
      "Index 19:  free\n",
      "Index 20:  to\n",
      "Index 21:  come\n",
      "Index 22:  back\n",
      "Index 23:  anytime\n",
      "Index 24: !\n",
      "Index 25:  Have\n",
      "Index 26:  a\n",
      "Index 27:  great\n",
      "Index 28:  day\n",
      "Index 29: !\n",
      "Index 30:  Bye\n",
      "Index 31: !\n",
      "Index 32: \n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "model = ChatOllama(model = \"llama3.1\")\n",
    "\n",
    "completion = model.invoke('Hi there!')\n",
    "print(f'Completion: {completion.content}')\n",
    "\n",
    "completions = model.batch(['Hi there!','Bye!'])\n",
    "print(f'Completions: {completions}')\n",
    "\n",
    "for index, token in enumerate(model.stream('Bye!'),start=1):\n",
    "    print(f'Index {index}: {token.content}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **invoke()** takes a single input and returns a single output.\n",
    "- **batch()** takes a list of inputs and returns a list of outputs.\n",
    "- **stream()** takes a single input and returns an iterator of parts of the output as they become available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components can be combined from 2 ways:\n",
    "- **Imperative**: Call your components directly. For instance, you can call a component's **invoke()** method directly.\n",
    "- **Declarative**: Use a **LangChain Expression Language (LCEL)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using Imperative Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Several model providers offer Large Language Models (LLMs). Here are some of the most notable ones:\\n\\n1. **Hugging Face Transformers**: Hugging Face provides a wide range of pre-trained transformer models, including BERT, RoBERTa, and', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-03-04T23:49:38.8767468Z', 'done': True, 'done_reason': 'length', 'total_duration': 9361596500, 'load_duration': 23307300, 'prompt_eval_count': 29, 'prompt_eval_duration': 251000000, 'eval_count': 50, 'eval_duration': 9085000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-6a06bdfa-4835-4e87-b4af-5d9afc21e27f-0', usage_metadata={'input_tokens': 29, 'output_tokens': 50, 'total_tokens': 79})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "# Building blocks\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system','You are a helpful assistant.'),\n",
    "    ('human','{question}'),\n",
    "])\n",
    "\n",
    "model = ChatOllama(model = \"llama3.1\", num_predict = 50)\n",
    "\n",
    "# combine them into a fuction\n",
    "# @chain decorator adds the same Runnable interface for any function you write\n",
    "# Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step.\n",
    "# https://python.langchain.com/v0.1/docs/modules/chains/\n",
    "\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    return model.invoke(prompt)\n",
    "\n",
    "# use it\n",
    "\n",
    "chatbot.invoke({\"question\":\"Which model providers offer LLMs?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example is a complete example of a chatbot, using a prompt and chat model. Uses the familiar Python syntax and supports any custom logic you might want to add in that function. If you would like to instead use **stream** method, you can use the **async** keyword in the function definition. Here it is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Several\n",
      " models\n",
      " provide\n",
      " Large\n",
      " Language\n",
      " Models\n",
      " (\n",
      "LL\n",
      "Ms\n",
      ")\n",
      " that\n",
      " can\n",
      " be\n",
      " fine\n",
      "-t\n",
      "uned\n",
      " for\n",
      " various\n",
      " N\n",
      "LP\n",
      " tasks\n",
      ".\n",
      " Here\n",
      " are\n",
      " some\n",
      " popular\n",
      " ones\n",
      ":\n",
      "\n",
      "\n",
      "1\n",
      ".\n",
      " **\n",
      "H\n",
      "ugging\n",
      " Face\n",
      " Transformers\n",
      "**:\n",
      " H\n",
      "ugging\n",
      " Face\n",
      " offers\n",
      " pre\n",
      "-trained\n",
      " transformer\n",
      " models\n",
      ",\n",
      " including\n",
      " popular\n",
      " ones\n",
      " like\n",
      " B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    for token in model.stream(prompt):\n",
    "        yield token\n",
    "\n",
    "for part in chatbot.stream({\n",
    "   \"question\": \"Which model providers offer LLMs?\" \n",
    "}):\n",
    "    print(part.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The model generated an incomplete response, mainly caused by the **num_predict** parameter also known as **max_tokens**. This parameter limits the size of the output, so a low value can cause the output generation to stop prematurely, so it may appear truncated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, for asynchronous execution, you can rewrite the function like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Several model providers offer Large Language Models (LLMs). Here are some of the most notable ones:\\n\\n1. **Hugging Face Transformers**: Hugging Face is one of the leading providers of pre-trained models, including many popular LLMs like B', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-03-05T00:03:47.0968816Z', 'done': True, 'done_reason': 'length', 'total_duration': 12932700900, 'load_duration': 2743405400, 'prompt_eval_count': 29, 'prompt_eval_duration': 1820000000, 'eval_count': 50, 'eval_duration': 7831000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-f1077f35-3a40-4054-9f65-b1e67876c2ae-0', usage_metadata={'input_tokens': 29, 'output_tokens': 50, 'total_tokens': 79})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice method ainvoke, used with asynchronous code\n",
    "@chain\n",
    "async def chatbot(values):\n",
    "    prompt = await template.ainvoke(values)\n",
    "    return await model.ainvoke(prompt)\n",
    "\n",
    "await chatbot.ainvoke({\"question\":\"Which model providers offer LLMs?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declarative Composition\n",
    "*LCEL* is a declarative language for composing LangChain components. LangChain compiles  LCEL compositions to an *optimized execution plan*, with automatic parallelization, streaming, tracing and async support. Here is an example of a LCEL composition:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Several model providers offer Large Language Models (LLMs):\\n\\n1. Hugging Face - Offers pre-trained models such as BERT and RoBERTa.\\n2. NVIDIA - Provides the Megatron-Turing NLG model and other transformer-based architectures.\\n3', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-03-05T00:34:22.703456Z', 'done': True, 'done_reason': 'length', 'total_duration': 8972744800, 'load_duration': 22761300, 'prompt_eval_count': 32, 'prompt_eval_duration': 1033000000, 'eval_count': 50, 'eval_duration': 7915000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-7588f13e-73f6-4d71-91fb-094ac66d06c9-0', usage_metadata={'input_tokens': 32, 'output_tokens': 50, 'total_tokens': 82})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# the building blocks\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system','You are a helpful assistant, give concise answers'),\n",
    "    ('human','{question}')\n",
    "])\n",
    "\n",
    "model = ChatOllama(model = 'llama3.1', num_predict=50)\n",
    "\n",
    "# combine them with the | operator\n",
    "\n",
    "chatbot = template | model\n",
    "\n",
    "# use it\n",
    "\n",
    "chatbot.invoke({\"question\":\"which model providers offer LLMs?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically this chain the template with the model. This means that the input is passed first to the template then the model generates a response based on the formatted prompt. The output is then returned to the user.\n",
    "\n",
    "Here is a basic example using instead the **stream** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Several' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' models' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' provide' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' Large' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' Language' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' Models' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' (' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='LL' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='Ms' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='):\\n\\n' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='1' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' **' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='H' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='ugging' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' Face' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' Transformers' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='**:' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' Offers' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' pre' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='-trained' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' transformer' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' models' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' for' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' N' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='LP' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' tasks' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='.\\n' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='2' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' **' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='Google' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' Cloud' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' AI' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' Platform' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='**:' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' Provides' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' pre' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='-trained' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' and' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' customizable' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' L' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='LM' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='s' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' for' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' text' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' generation' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' and' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' other' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content=' applications' additional_kwargs={} response_metadata={} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13'\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2025-03-05T00:34:42.1714162Z', 'done': True, 'done_reason': 'length', 'total_duration': 9147703300, 'load_duration': 31332200, 'prompt_eval_count': 32, 'prompt_eval_duration': 1096000000, 'eval_count': 50, 'eval_duration': 8018000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-c5f36d28-af0b-406c-9136-45b499ec8c13' usage_metadata={'input_tokens': 32, 'output_tokens': 50, 'total_tokens': 82}\n"
     ]
    }
   ],
   "source": [
    "chatbot = template | model\n",
    "for part in chatbot.stream({\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "}):\n",
    "    print(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For asynchronous execution, you can rewrite the function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Several major models provide Large Language Models (LLMs), including:\\n\\n1. Hugging Face Transformers\\n2. Meta AI's LLaMA\\n3. Google's PaLM\\n4. OpenAI's GPT-3\\n5. Microsoft's\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-03-05T00:35:44.4353493Z', 'done': True, 'done_reason': 'length', 'total_duration': 8074851800, 'load_duration': 31823400, 'prompt_eval_count': 32, 'prompt_eval_duration': 343000000, 'eval_count': 50, 'eval_duration': 7699000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-042c9c2e-6c23-44ce-9666-0f8891dfeada-0', usage_metadata={'input_tokens': 32, 'output_tokens': 50, 'total_tokens': 82})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot = template | model\n",
    "await chatbot.ainvoke({\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to the [README.md](README.md/#rags) file to see the next steps. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
