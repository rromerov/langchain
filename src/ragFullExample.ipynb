{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the indexing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "# Load the document, split it inot chunks\n",
    "raw_documents = TextLoader('../data/test.txt').load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,\n",
    "                                               chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# embed each chunk and insert it inot the vector store\n",
    "model = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "connection = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\n",
    "db = PGVector.from_documents(documents,model, connection=connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indexing stage is complete. To perform the retrieval stage, we need to calculate similarity search calculations between the query and the documents, so relevant chunks from our indexed documented are retrieved. Retrieval process follows this approach:\n",
    "\n",
    "1. Convert the user's query into embeddings.\n",
    "2. Calculate the embeddings in the vector store that are most similar to the user's query.\n",
    "3. Retrieve the relevant document embeddings and their corresponding text chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retriever\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# fetch relevant documents\n",
    "docs = retriever.invoke(\"\"\"Who are the key figures in the ancient greek history of philosophy?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using *.as_retriever* method, this is used to abstract the logic of embedding the user's query and the underlysing similarity search calculations performed by the vector store to retrieve the relevant documents. There's also a argument *k*, that determines the number of relevant documents to fetch from the vector store. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retriever with k=2\n",
    "retriever = db.as_retriever(search_kwargs={\"k\":2})\n",
    "\n",
    "# fetch the 2 most relevant documents\n",
    "docs = retriever.invoke(\"\"\"Who are the key figures in the ancient greek history \n",
    "    of philosophy?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we have selected 2 as the value of **k**. This tells the vector store to return the two most relevant documents based on the user's query. Adding more documents can slower application perfomance, also it would make the prompt larger (also associated cost of generation) will be, and the greater the likelihodd of retrieving chunks of texts that contain irrelevant information, which could lead to LLM hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating LLM Predictions Using Relevant Documents\n",
    "Once there are relevant documents based on the user's query, the final step is to add them to the original prompt as context and then invoke the model to generate a final output. As you can see in the following image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LLM flow with RAG](../img/LLMflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will serve as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The Eternal Voyage appears to be a poetic and metaphorical journey through time, exploring themes of fate, memory, loss, and the passage of time. The poem describes a vessel sailing across \"endless seas of time,\" with a captain who navigates through \"mist-clad isles of memory\" and charts a course through \"fate unknown.\" The poem also touches on the idea of empires rising and falling, and the fleeting nature of human endeavors.\\n\\nThe overall tone of the poem suggests that it is about the human experience of navigating through time, confronting the impermanence of things, and seeking to understand what lingers beyond death. The poem\\'s use of imagery and symbolism creates a sense of mystery and wonder, inviting the reader to reflect on their own place in the passage of time.\\n\\nWithout more context or information about the author\\'s intentions, it is difficult to provide a definitive interpretation of the Eternal Voyage. However, based on the language and themes used in the poem, it appears to be a contemplative and introspective work that explores the human condition through the metaphor of a journey across time.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-03-29T18:47:37.3356115Z', 'done': True, 'done_reason': 'stop', 'total_duration': 70269899800, 'load_duration': 10242577400, 'prompt_eval_count': 1153, 'prompt_eval_duration': 14651000000, 'eval_count': 223, 'eval_duration': 45371000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-89c56dab-439d-4fe5-a3e0-e1becf28cbd6-0', usage_metadata={'input_tokens': 1153, 'output_tokens': 223, 'total_tokens': 1376})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on\n",
    "                                          the following context:\n",
    "                                          {context}\n",
    "                                          \n",
    "                                          Question: {question}\n",
    "                                          \"\"\")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "# fetch relevant documents\n",
    "# docs = retriever.get_relevant_documents(\"\"\"What is the eternal voyage about?\"\"\")\n",
    "# note this method was deprecated, suggestion is to use instead invoke method. Adjusting code\n",
    "docs = retriever.invoke(\"\"\"What is the eternal voyage about?\"\"\")\n",
    "\n",
    "# run\n",
    "chain.invoke({\"context\": docs, \"question\": \"\"\"What is the eternal voyage about?\"\"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief explanation of the code above:\n",
    "- We implemented a dynamic *context* and *question* variables into our prompt, which allow us to define a *ChatPromptTemplate* the model can use to generate responses.\n",
    "- We define a *ChatOllama* interface to act as our LLM. Temperature is set to 0 to eliminate the creativity in outputs from the model.\n",
    "- We create a chain to compose the prompt and LLM.\n",
    "- We *invoke* the cain passing in the *context* variable (our retrieved relevant docs) and the user's question to generate a final input.\n",
    "\n",
    "We can encapsulate this retrieeval logic in a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Eternal Voyage appears to be a poetic and metaphorical journey through time, exploring themes of fate, memory, loss, and the passage of time. The poem describes a vessel sailing across \"endless seas of time,\" with a captain who navigates through \"mist-clad isles of memory\" and charts a course through \"fate unknown.\" The poem also touches on the idea of empires rising and falling, and the fleeting nature of human endeavors.\\n\\nThe overall tone of the poem suggests that it is about the human experience of navigating through time, confronting the impermanence of things, and seeking to understand what lingers beyond death. The poem\\'s use of imagery and symbolism creates a sense of mystery and wonder, inviting the reader to reflect on their own place in the passage of time.\\n\\nThe repetition of similar lines across different documents suggests that this is a central theme or idea being explored, rather than a specific narrative or plot.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on\n",
    "                                            the following context:\n",
    "                                          {context}\n",
    "\n",
    "                                          Question: {question}\n",
    "                                          \"\"\")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "@chain\n",
    "def qa(input: str):\n",
    "    # fetch the relevant documents\n",
    "    docs = retriever.invoke(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer.content\n",
    "\n",
    "#run\n",
    "qa.invoke(\"What is the eternal voyage about?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a runnable **qa** function, that can be called with just a question and takes care to first fetch the relevant docs for context, format them into the prompt, and finally generate the answer. *@chain* decorator turns the function into a runnable chain. This notion of encapsulating multiple steps into a single function is key to building interesting apps with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='The Eternal Voyage appears to be a poetic and metaphorical journey through time, exploring themes of fate, memory, loss, and the passage of time. The poem describes a vessel sailing across \"endless seas of time,\" with a captain who navigates through \"mist-clad isles of memory\" and charts a course through \"fate unknown.\" The poem also touches on the idea of empires rising and falling, and the fleeting nature of human endeavors.\\n\\nThe overall tone of the poem suggests that it is about the human experience of navigating through time, confronting the impermanence of things, and seeking to understand what lingers beyond death. The poem\\'s use of imagery and symbolism creates a sense of mystery and wonder, inviting the reader to reflect on their own place in the passage of time.\\n\\nThe repetition of similar lines across different documents suggests that this is a central theme or idea being explored, rather than a specific narrative or plot.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-03-29T18:49:44.5799849Z', 'done': True, 'done_reason': 'stop', 'total_duration': 61980337400, 'load_duration': 8029121200, 'prompt_eval_count': 1151, 'prompt_eval_duration': 14110000000, 'eval_count': 189, 'eval_duration': 39837000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-7b59546b-e81b-4efd-9268-848719b7ffa8-0', usage_metadata={'input_tokens': 1151, 'output_tokens': 189, 'total_tokens': 1340}),\n",
       " 'docs': [Document(id='6d5e9d43-19b9-4865-bff8-cdd559035125', metadata={'source': '../data/test.txt'}, page_content='The Eternal Voyage\\n\\nAcross the endless seas of time,\\nWhere echoes fade, where stars align,\\nA vessel sails with silver light,\\nA phantom ship in endless flight.\\n\\nThe captain stands with eyes aglow,\\nA soul of fire, a heart of snow,\\nHis compass spins, yet finds no ground,\\nLost in whispers, lost in sound.\\n\\nThe ocean sings in endless tides,\\nOf kings once fallen, hopes that rise,\\nOf lovers lost in stormy waves,\\nAnd those who dream beyond their graves.\\n\\nThrough mist-clad isles of memory,\\nWhere silent ghosts drift ceaselessly,\\nHe charts a course through fate unknown,\\nA mariner by stars alone.\\n\\nThe Isles of Lost Tomorrows\\n\\nThe wind it calls in spectral tones,\\nTo lands where golden ages shone,\\nWhere empires rose, then turned to dust,\\nBound to timeâ€™s unyielding thrust.\\n\\nUpon the shores of shattered fate,\\nThe footprints fade, the echoes wait,\\nFor those who walk with fearless stride,\\nTo learn what lingers, what has died.'),\n",
       "  Document(id='361f6204-50e2-4ff9-9bb2-3f789ceec90c', metadata={'source': '../data/test.txt'}, page_content='The Eternal Voyage\\n\\nAcross the endless seas of time,\\nWhere echoes fade, where stars align,\\nA vessel sails with silver light,\\nA phantom ship in endless flight.\\n\\nThe captain stands with eyes aglow,\\nA soul of fire, a heart of snow,\\nHis compass spins, yet finds no ground,\\nLost in whispers, lost in sound.\\n\\nThe ocean sings in endless tides,\\nOf kings once fallen, hopes that rise,\\nOf lovers lost in stormy waves,\\nAnd those who dream beyond their graves.\\n\\nThrough mist-clad isles of memory,\\nWhere silent ghosts drift ceaselessly,\\nHe charts a course through fate unknown,\\nA mariner by stars alone.\\n\\nThe Isles of Lost Tomorrows\\n\\nThe wind it calls in spectral tones,\\nTo lands where golden ages shone,\\nWhere empires rose, then turned to dust,\\nBound to timeâ€™s unyielding thrust.\\n\\nUpon the shores of shattered fate,\\nThe footprints fade, the echoes wait,\\nFor those who walk with fearless stride,\\nTo learn what lingers, what has died.'),\n",
       "  Document(id='29983c37-613f-4ee7-80d5-2bcf299dbc81', metadata={'source': './test.txt'}, page_content='The Eternal Voyage\\n\\nAcross the endless seas of time,\\nWhere echoes fade, where stars align,\\nA vessel sails with silver light,\\nA phantom ship in endless flight.\\n\\nThe captain stands with eyes aglow,\\nA soul of fire, a heart of snow,\\nHis compass spins, yet finds no ground,\\nLost in whispers, lost in sound.\\n\\nThe ocean sings in endless tides,\\nOf kings once fallen, hopes that rise,\\nOf lovers lost in stormy waves,\\nAnd those who dream beyond their graves.\\n\\nThrough mist-clad isles of memory,\\nWhere silent ghosts drift ceaselessly,\\nHe charts a course through fate unknown,\\nA mariner by stars alone.\\n\\nThe Isles of Lost Tomorrows\\n\\nThe wind it calls in spectral tones,\\nTo lands where golden ages shone,\\nWhere empires rose, then turned to dust,\\nBound to timeâ€™s unyielding thrust.\\n\\nUpon the shores of shattered fate,\\nThe footprints fade, the echoes wait,\\nFor those who walk with fearless stride,\\nTo learn what lingers, what has died.'),\n",
       "  Document(id='461aa62f-9132-4d7e-aced-8132b263dd13', metadata={'source': '../data/test.txt'}, page_content='The Eternal Voyage\\n\\nAcross the endless seas of time,\\nWhere echoes fade, where stars align,\\nA vessel sails with silver light,\\nA phantom ship in endless flight.\\n\\nThe captain stands with eyes aglow,\\nA soul of fire, a heart of snow,\\nHis compass spins, yet finds no ground,\\nLost in whispers, lost in sound.\\n\\nThe ocean sings in endless tides,\\nOf kings once fallen, hopes that rise,\\nOf lovers lost in stormy waves,\\nAnd those who dream beyond their graves.\\n\\nThrough mist-clad isles of memory,\\nWhere silent ghosts drift ceaselessly,\\nHe charts a course through fate unknown,\\nA mariner by stars alone.\\n\\nThe Isles of Lost Tomorrows\\n\\nThe wind it calls in spectral tones,\\nTo lands where golden ages shone,\\nWhere empires rose, then turned to dust,\\nBound to timeâ€™s unyielding thrust.\\n\\nUpon the shores of shattered fate,\\nThe footprints fade, the echoes wait,\\nFor those who walk with fearless stride,\\nTo learn what lingers, what has died.')]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@chain\n",
    "def qa(input: str):\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.invoke(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return {\"answer\": answer, \"docs\": docs}\n",
    "\n",
    "#run\n",
    "qa.invoke(\"What is the eternal voyage about?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a basic RAG system to power an AI app for personal use. However a production ready AI app used by multiple users requires a more advanced RAG system. To do so we require the following:\n",
    "\n",
    "1. How do we handle the variability in the quality of a user's input?\n",
    "2. How do we route queries to retrieve relevant data from a variety of data sources?\n",
    "3. How do we transform natural language to the query language of the target data source?\n",
    "4. How do we optimize our indexing process, i.e., embedding, text splitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dive into one of the relevant topics regarding advanced RAG systems:\n",
    "\n",
    "## Query Transformation\n",
    "One of the main issues that RAG systems have is that they relay heavily on the quality of a user input. In a production environment, is likely that a user input is incomplete, ambiguous, or poorly worder manner that leads to model hallucination.\n",
    "\n",
    "Query transformation is a subset of strategies designed to modify the user's input to answer the first question: *How do we handle the variability in the quality of a user's input?*\n",
    "\n",
    "One strategy could be as follows:\n",
    "\n",
    "### Rewrite-Retrieve-Read\n",
    "Basically this strategy involves LLM to rewrite the user's input before performing the retrieval. First let's see a poor written prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The context provided doesn\\'t mention anything about your daily activities or the news. It appears to be a collection of documents with poetic content.\\n\\nAs for \"The Eternal Voyage,\" it seems to be a poem that describes a journey through time, space, and memory. The poem uses metaphors and imagery to convey themes of exploration, loss, and the passage of time.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-03-29T18:50:23.9509807Z', 'done': True, 'done_reason': 'stop', 'total_duration': 37413659200, 'load_duration': 8013306500, 'prompt_eval_count': 1181, 'prompt_eval_duration': 14966000000, 'eval_count': 74, 'eval_duration': 14433000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-06a5c26a-67b9-4e98-aabf-90c53c706819-0', usage_metadata={'input_tokens': 1181, 'output_tokens': 74, 'total_tokens': 1255})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@chain\n",
    "def qa(input):\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.invoke(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "qa.invoke(\"\"\"Today I woke up and brushed my teeth, then I sat down to read the \n",
    "    news. But then I forgot the food on the cooker. What is the eternal voyage about?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model failed to answer because it got distracted by irrelevant information provided in the user's query. Let's implement the Rewrite-Retrieve-Read prompt.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The Eternal Voyage poem appears to be a metaphorical and poetic piece that explores themes of time, memory, fate, and the human experience. The poem describes a vessel sailing through \"endless seas of time,\" where echoes fade and stars align.\\n\\nBased on the content, it seems that the poem is about a journey through the past, present, and future, where the speaker navigates through memories, lost moments, and forgotten eras. The poem touches on ideas of impermanence, the passage of time, and the human condition.\\n\\nThe poem\\'s structure and language suggest a sense of longing, nostalgia, and wonder. The speaker seems to be drawn to the idea of exploring the unknown, charting a course through fate, and learning from the past.\\n\\nSome possible interpretations of the poem include:\\n\\n* A reflection on the fleeting nature of time and human experience\\n* An exploration of the power of memory and its relationship to our understanding of ourselves and the world around us\\n* A metaphorical journey through the realms of history, where the speaker encounters echoes of the past and lost moments in time\\n\\nOverall, the poem\\'s meaning is open to interpretation, but it appears to be a contemplative and introspective piece that invites readers to reflect on their own place within the vast expanse of time.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-03-29T18:52:05.891551Z', 'done': True, 'done_reason': 'stop', 'total_duration': 78134819300, 'load_duration': 7766913700, 'prompt_eval_count': 1185, 'prompt_eval_duration': 15167000000, 'eval_count': 261, 'eval_duration': 55199000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-38a1eeb4-413b-4a3a-95e4-c442c3f27fe6-0', usage_metadata={'input_tokens': 1185, 'output_tokens': 261, 'total_tokens': 1446})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewrite_prompt = ChatPromptTemplate.from_template(\"\"\"Provide a better search \n",
    "    query for web search engine to answer the given question, end the queries \n",
    "    with ’**’. Question: {x} Answer:\"\"\")\n",
    "\n",
    "def parse_rewriter_output(message):\n",
    "    return message.content.strip('\"').strip(\"**\")\n",
    "\n",
    "rewriter = rewrite_prompt | llm | parse_rewriter_output\n",
    "\n",
    "@chain\n",
    "def qa_rrr(input):\n",
    "    # rewrite the query\n",
    "    new_query = rewriter.invoke(input)\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.invoke(new_query)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "# run\n",
    "qa_rrr.invoke(\"\"\"Today I woke up and brushed my teeth, then I sat down to read the \n",
    "    news. But then I forgot the food on the cooker. Nevermind, What is the eternal voyage poem about?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we had an LLM rewriter the user's initial distracted query into a much clearer one, and it is that more focused query that is passed to the retriever to fetch the most relevant documents. \n",
    "\n",
    "> **Note:** This technique can be used with any retrieval method, , be that a vector store such as we have here or, for instance, a web search tool. The downside of this approach is that it introduces additional latency into our chain, because now we need to perform 2 LLM calls in sequence.\n",
    "\n",
    "### Multi-Query Retrieval\n",
    "A user's single query can be insufficient to capture the full scope of information required to answer the query comprehensively.The multiquery retrieval strategy resolves this problem by instructing an LLM to generate multiple queries based on a user's initial query, executing a parallel retrieval of each query from the data source and then insterting the retrieved results as prompt context to generate a final model output. Here you can see an image of the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multi-Query Retrieval](../img/Multi-Query-Retrieval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strategy is helpful when a single question may rely on multiple perspectives to provide a comprehensive answer. Here is a code eample of multi-query retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "perspectives_prompt = ChatPromptTemplate.from_template(\"\"\"You are an AI language \n",
    "    model assistant. Your task is to generate five different versions of the \n",
    "    given user question to retrieve relevant documents from a vector database. \n",
    "    By generating multiple perspectives on the user question, your goal is to \n",
    "    help the user overcome some of the limitations of the distance-based \n",
    "    similarity search. Provide these alternative questions separated by \n",
    "    newlines. Original question: {question}\"\"\")\n",
    "\n",
    "def parse_queries_output(message):\n",
    "    return message.content.split('\\n')\n",
    "\n",
    "query_gen = perspectives_prompt | llm | parse_queries_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prompt template is aimed at generating variations of questions based on the user's initial query. We then take the list of generated queries, retrieve the most relevant docs for each of them in parallel, and then combine to get the unique union of all the retrieved relevant documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_union(document_lists):\n",
    "    # Flatten list of lists, and dedupe them\n",
    "    deduped_docs = {\n",
    "        doc.page_content: doc\n",
    "        for sublist in document_lists for doc in sublist\n",
    "    }\n",
    "\n",
    "    # return a flat list of unique docs\n",
    "    return list(deduped_docs.values())\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | get_unique_union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're retrieving documents from the same retriever with multiple (related) queries, it's likely at least some of them are repeated. Before using them as context to answer the question, we need to deduplicate them, to end up with a single instance of each. Here we dedupe docs by using their content (a string) as the key in a dictionary, because a dictionary only contain one entry for each key. After we've iterated through all docs, we simply get all the dictionary values, which is now free of duplicates.\n",
    "\n",
    "Notice that we are using **.batch** as well, which runs all generated queries in parallel and returns a list of results-in this case, a list of lists of documents, which we then flatten and dedupe.\n",
    "\n",
    "The final step is to construct a prompt, including the user's question and combined retrieved relevant documents, and a model interface to generate the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The \"Eternal Voyage\" poem appears to be a continuation of the themes and motifs introduced in the previous poem, \"The Forest of Forgotten Names\". It seems to explore the idea of a journey through time, space, and memory.\\n\\nAt its core, the poem is about a vessel (representing the human soul or spirit) that sails across the \"endless seas of time\", navigating through the echoes of the past, guided by the light of memories and the whispers of those who have come before. The captain of this vessel is on a quest to chart a course through fate unknown, using only the stars as his guide.\\n\\nThe poem touches on various themes, including:\\n\\n1. **Loss and remembrance**: The poem acknowledges that things are lost in time, but also suggests that memories linger, waiting to be rediscovered.\\n2. **Journeying through time**: The vessel\\'s journey across the seas of time implies a exploration of the past, present, and future.\\n3. **The power of memory**: Memories are portrayed as a guiding force, helping the captain navigate the unknown.\\n4. **The search for meaning**: The poem suggests that the journey is not just about reaching a destination but also about understanding the significance of what has been lost.\\n\\nOverall, \"The Eternal Voyage\" can be seen as an exploration of the human experience, where individuals embark on a journey through time and memory to make sense of their place in the world.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-03-29T18:53:49.6102649Z', 'done': True, 'done_reason': 'stop', 'total_duration': 74815103800, 'load_duration': 8044321700, 'prompt_eval_count': 613, 'prompt_eval_duration': 7539000000, 'eval_count': 291, 'eval_duration': 59227000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-6a13acf1-8fc7-4bdd-9f1f-8a6b2b5431b6-0', usage_metadata={'input_tokens': 613, 'output_tokens': 291, 'total_tokens': 904})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based\n",
    "                                          on this context:\n",
    "                                          \n",
    "                                          {context}\n",
    "                                          \n",
    "                                          Question: {question}\n",
    "                                          \"\"\")\n",
    "\n",
    "@chain\n",
    "def multi_query_qa(input):\n",
    "    # fetch relevant documents\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "# run\n",
    "multi_query_qa.invoke(\"\"\"What is the eternal voyage poem about?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chain isn't too different from previous chains, as all the new logic for multi-query retrieval is contained in **retrieval_chain**- This is key to making good use of these techniques as a standalone chain (in this case, **retrieval_chain**), which makes it easy to adopt them and enven to combine them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG-Fusion\n",
    "This strategy shares similarities with the multi-query retrieval strategy, except we will apply a final reranking step to all the retrieved docuemtns. This reranking steps use *reciprocal rank fusion* (RRF) algorithm, which involves combining the ranks of different search results to produce a single, unified ranking. By combining ranks from different queries, we pull the most relevant documents to the top of the final list. RRF is well-suited for combining results from queries that migh have different scales or distributions of scores. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "propmt_rag_fusion = ChatPromptTemplate.from_template(\"\"\"You are a helpful\n",
    "                                                     assistant that generates multiple search queries based on a single input\n",
    "                                                     query. \\n\n",
    "                                                     Generate multiple search queries related to: {question} \\n\n",
    "                                                     Output (4 queries):\"\"\")\n",
    "\n",
    "def parse_queries_output(message):\n",
    "    return message.content.split('\\n')\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "query_gen = propmt_rag_fusion | llm |   parse_queries_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've generated our queries, we fetch relevant documents for each query and pass them into a function to *rerank* (*reorder* according to relevancy) the final list of relevant docuemtns. The function **recirpocal_rank_fusion** takes a list of the search resultas of each query, so a list of lists of documents, where each inner list of documents is sorted by their relevance so that query. The RRF algorithm then calculates a new score for each document based on its ranks (or positions) in the different lists and sorts them to create a final reranked list.\n",
    "\n",
    "After calculating the fused scores, the function sorts the documents in descending order of these scores to get the final reranked list, which is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recicprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"\"reciprocal rank fusion on multiple lists of ranked documents\n",
    "    and an optimal parameter k used in the RRF formula\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to hold fused scores for each document\n",
    "    # Documents will be keyed by their contents to ensure uniqueness\n",
    "    fused_scores = {}\n",
    "    documents = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each docuemnt in the list,\n",
    "        # with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Use the document contents as the key for uniqueness\n",
    "            doc_str = doc.page_content\n",
    "            # If the document hasn't been seen yet,\n",
    "            # - initialize score to 0\n",
    "            # - save it for later\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "                documents[doc_str] = doc\n",
    "            # Update the socre of the document using the RRF formula\n",
    "            # 1 / (rank + k) \n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    \n",
    "    # Sort the documents based on their fused scores in descending order\n",
    "    # to get the final reranked results\n",
    "    reranked_docs_strs = sorted(\n",
    "        fused_scores, key=lambda d: fused_scores[d], reverse=True\n",
    "    )\n",
    "\n",
    "    # retrieve the corresponding doc for each doc_str\n",
    "    return [\n",
    "        documents[doc_str]\n",
    "        for doc_str in reranked_docs_strs\n",
    "    ]\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | recicprocal_rank_fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the function also takes a *k* parameter, used to determine how much influence documents in each query's result sets have over the final list of documents. A *higher value of k* means that lower-ranked documents have more influence.\n",
    "\n",
    "Finally, we combine our new retrieval chain (now using RRF) with the full chain we've seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The \"Eternal Voyage\" poem appears to be a philosophical and metaphorical exploration of the human experience, mortality, and the passage of time. On its surface, it describes a journey across the seas of time, where a vessel sails with silver light, guided by a captain who is lost in whispers and sound.\\n\\nHowever, upon closer reading, the poem reveals themes of:\\n\\n1. **The fleeting nature of life**: The poem touches on the idea that everything is transient, including human existence. The', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-03-29T18:55:43.5837186Z', 'done': True, 'done_reason': 'length', 'total_duration': 37282589300, 'load_duration': 7778347100, 'prompt_eval_count': 895, 'prompt_eval_duration': 10809000000, 'eval_count': 100, 'eval_duration': 18694000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-3309e9a4-1cdb-4c8c-a76d-85991a3aff35-0', usage_metadata={'input_tokens': 895, 'output_tokens': 100, 'total_tokens': 995})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based\n",
    "                                          on this context:\n",
    "                                          \n",
    "                                          {context}\n",
    "                                          \n",
    "                                          Question: {question}\n",
    "                                          \"\"\")\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0, num_predict=100)\n",
    "\n",
    "@chain\n",
    "def multi_query_qa(input):\n",
    "    # fetch relevant documents\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "multi_query_qa.invoke(\"What is the eternal voyage poem about?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG-Fusion's strength lies in its ability to capture the user's intended expression, navigate complex queries, and broaden the scope of retrieved documents, enabling serendipitous discovery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothetical Document Embeddings\n",
    "*Hypothetical Document Embeddings (HyDE)* is a strategy that involves creating a hyphotetical document based on the user's query, embedding the document, and retrieving relevant documents based on vector similarity. The intuition behind HyDE is that an LLM-generated hypothetical document will be more similar to the most relevant documents than the original query. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "prompt_hyde = ChatPromptTemplate.from_template(\"\"\"Please write a passage to\n",
    "                                               answer the question. \\n Question: {question} \\n Passage:\"\"\")\n",
    "\n",
    "generate_doc = (\n",
    "    prompt_hyde | ChatOllama(model=\"llama3.1\", temperature=0, num_predict=100) | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we take the hypothetical document and use it as an input to the **retriever**, which will generate its embedding and search for similar documents in the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = generate_doc | retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we take the retrieved documents, pass them as context to the final prompt, and instruct the model to generate an output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The \"Eternal Voyage\" poem appears to be a philosophical and metaphorical exploration of time, fate, memory, and the human experience. On its surface, it describes a journey across the seas of time, where a vessel sails with silver light, guided by a captain who is lost in whispers and sound.\\n\\nHowever, upon closer reading, the poem reveals themes of:\\n\\n1. **The passage of time**: The poem acknowledges that echoes fade, stars align, and empires rise and fall, emphasizing', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-03-29T19:27:20.4617645Z', 'done': True, 'done_reason': 'length', 'total_duration': 42287971500, 'load_duration': 8060673000, 'prompt_eval_count': 1152, 'prompt_eval_duration': 13889000000, 'eval_count': 100, 'eval_duration': 20336000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-79126f6f-ef28-45b1-a57c-709567794bc3-0', usage_metadata={'input_tokens': 1152, 'output_tokens': 100, 'total_tokens': 1252})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based on this context:\n",
    "                                          \n",
    "                                          {context}\n",
    "                                          \n",
    "                                          Question: {question}\"\"\")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0, num_predict=100)\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "    # fetch relevant documents from the hyde retrieval chain defined earlier\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "qa.invoke(\"What is the eternal voyage poem about?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At glance **query transformation** consists of taking the user's original query and:\n",
    "1. Rewrite it into one or more queries.\n",
    "2. Combining the results of those queries into a single set of the most relevant results.\n",
    "\n",
    "This can take several forms, usually the process is to use the user's original query and ask the LLM model to write a new query or queries. Some examples of typical changes made are:\n",
    "\n",
    "- Removing irrelevant/unrelated text from the query.\n",
    "- Grounding the query with past conversation history. \n",
    "- Casting a wider net for relevant documents by also fetching documents for related queries.\n",
    "- Decomposing a complex question into multiple, simpler questions and then including results for all of them in the final prompt to generate an answer.\n",
    "\n",
    "The right rewritting strategy to use will depend on your use case.\n",
    "\n",
    "In order to develop robust RAG system, we also need to route queries to retrieve relevant data from multiple data sources, for that we use Query Routing. Let's go back to [README file](../README.md/#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
