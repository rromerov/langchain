{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa10eb5",
   "metadata": {},
   "source": [
    "### **Testing an Agent’s Final Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c51ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Create a dataset\n",
    "# Create a dataset\n",
    "examples = [\n",
    "    (\"Which country's customers spent the most? And how much did they spend?\",\n",
    "        \"\"\"The country whose customers spent the most is the USA, with a total \n",
    "        expenditure of $523.06\"\"\"),\n",
    "    (\"What was the most purchased track of 2013?\", \n",
    "        \"The most purchased track of 2013 was Hot Girl.\"),\n",
    "    (\"How many albums does the artist Led Zeppelin have?\",\n",
    "        \"Led Zeppelin has 14 albums\"),\n",
    "    (\"What is the total price for the album “Big Ones”?\",\n",
    "        \"The total price for the album 'Big Ones' is 14.85\"),\n",
    "    (\"Which sales agent made the most in sales in 2009?\", \n",
    "        \"Steve Johnson made the most sales in 2009\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cdcf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"SQL Agent Response\"\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    inputs, outputs = zip(\n",
    "        *[({\"input\": text}, {\"output\": label}) for text, label in examples]\n",
    "    )\n",
    "    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)\n",
    "\n",
    "## chain\n",
    "def predict_sql_agent_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    msg = {\"messages\": (\"user\", example[\"input\"])}\n",
    "    messages = graph.invoke(msg, config)\n",
    "    return {\"response\": messages[\"messages\"][-1].content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b1736a",
   "metadata": {},
   "source": [
    "Now we can use the generated answer with the reference answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8bc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Grade prompt\n",
    "grade_prompt_answer_accuracy = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"A simple evaluator for RAG answer accuracy\"\"\"\n",
    "\n",
    "    # Get question, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs[\"input\"]\n",
    "    reference = example.outputs[\"output\"]\n",
    "    prediction = run.outputs[\"response\"]\n",
    "\n",
    "    # LLM grader \n",
    "    llm = AzureChatOpenAI(model=\"gpt-4o\", temperature=0, azure_deployment=\"gpt-4o\", api_version=\"2024-10-21\")\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm \n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"correct_answer\": reference,\n",
    "                                  \"student_answer\": prediction})\n",
    "    \n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}\n",
    "\n",
    "## RUn evaluation\n",
    "experiment_results = evaluate(\n",
    "    predict_sql_agent_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    num_repetitions=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35656486",
   "metadata": {},
   "source": [
    "**Testing a Single Step of an Agent**\n",
    "\n",
    "Evaluating individual decisions made by an agent helps pinpoint performance issues.\n",
    "\n",
    "**Typical Setup:**\n",
    "\n",
    "1. **Inputs**  \n",
    "   - User input to a specific step (e.g., user prompt, available tools)  \n",
    "   - May include previous steps for context\n",
    "\n",
    "2. **Output**  \n",
    "   - LLM-generated response for this step  \n",
    "   - Often includes tool call and input parameters\n",
    "\n",
    "3. **Evaluator**  \n",
    "   - Binary score for tool selection correctness  \n",
    "   - Heuristic evaluation of the tool input’s accuracy\n",
    "\n",
    "**Example:**  \n",
    "Use a custom evaluator to verify if the agent selected the correct tool and provided valid inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee94848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "def predict_assistant(example: dict):\n",
    "    \"\"\"Invoke assistant for single tool call evaluation\"\"\"\n",
    "    msg = [(\"user\", example[\"input\"])]\n",
    "    result = assistant_runnable.invoke({\"messages\":msg})\n",
    "    return {\"response\": result}\n",
    "\n",
    "def check_specific_tool_call(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Check if the first tool call in the response matches the expected tool call.\n",
    "    \"\"\"\n",
    "    # Expected tool call\n",
    "    expected_tool_call = 'sql_db_list_tables'\n",
    "\n",
    "    # Run\n",
    "    response = root_run.outputs[\"response\"]\n",
    "\n",
    "    # Get tool call\n",
    "    try:\n",
    "        tool_call = getattr(response, \"tool_calls\", [])[0][\"name\"]\n",
    "    except (IndexError, KeyError):\n",
    "        tool_call = None\n",
    "    \n",
    "    score = 1 if tool_call == expected_tool_call else 0\n",
    "    return {\"score\": score, \"key\": \"single_tool_call\"}\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_assistant,\n",
    "    data=dataset_name,\n",
    "    evaluators=[check_specific_tool_call],\n",
    "    num_repetitions=3,\n",
    "    metadata={\"version\": metadata},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd1fc3b",
   "metadata": {},
   "source": [
    "**Testing an Agent’s Trajectory**\n",
    "\n",
    "Analyzing the sequence of actions taken by an agent helps determine if the control flow aligns with expectations.\n",
    "\n",
    "**Typical Setup:**\n",
    "\n",
    "1. **Inputs**  \n",
    "   - User input  \n",
    "   - Optionally includes a predefined set of tools\n",
    "\n",
    "2. **Output**  \n",
    "   - Expected sequence of tool calls  \n",
    "   - Can also allow for unordered tool calls\n",
    "\n",
    "3. **Evaluator**  \n",
    "   - Custom function applied to the agent’s steps  \n",
    "   - Can use:\n",
    "     - Binary score for exact match\n",
    "     - Count of incorrect or extra steps\n",
    "   - For LLM-as-a-judge: pass the full trajectory as a message list and ask for an evaluation\n",
    "\n",
    "**Example:**  \n",
    "Compare the actual sequence of tool calls with the expected reference trajectory using a custom evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a664f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sql_agent(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    msg = {\"messages\": (\"user\", example[\"input\"])}\n",
    "    messages = graph.invoke(msg, config)\n",
    "    return {\"response\": messages}\n",
    "\n",
    "def find_tool_calls(messages):\n",
    "    \"\"\"Find all tool calls in the messages returned\"\"\"\n",
    "    tools_calls = [\n",
    "        tc[\"name\"]\n",
    "        for m in messages[\"messages\"] for tc in getattr(m, \"tools_calls\", [])\n",
    "    ]\n",
    "    return tools_calls\n",
    "\n",
    "def contains_all_tool_calls_any_order(\n",
    "        root_run: Run, example: Example\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Check if the response contains all expected tool calls in any order.\n",
    "    \"\"\"\n",
    "    # Expected tool calls\n",
    "    expected = [\n",
    "        'sql_db_list_tables',\n",
    "        'sql_db_schema',\n",
    "        'sql_db_query_checker',\n",
    "        'sql_db_query',\n",
    "        'check_result'\n",
    "    ]\n",
    "    messages = root_run.outputs[\"response\"]\n",
    "    tool_calls = find_tool_calls(messages)\n",
    "    # Optionally, log the tool calls -\n",
    "    # print(\"Here are my tool calls:\")\n",
    "    # print(tool_calls)\n",
    "    if set(expected) <= set(tool_calls):\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "    return {\"score\":int(score), \"key\": \"multi_tool_call_any_roder\"}\n",
    "\n",
    "def contains_all_tool_calls_in_order(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Check if all expected tools are called in exact order.\n",
    "    \"\"\"\n",
    "    messages = root_run.outputs[\"response\"]\n",
    "    tool_calls = find_tool_calls(messages)\n",
    "    # Optionally, log the tool calls -\n",
    "    #print(\"Here are my tool calls:\")\n",
    "    #print(tool_calls)\n",
    "    it = iter(tool_calls)\n",
    "    expected = [\n",
    "        'sql_db_list_tables', \n",
    "        'sql_db_schema', \n",
    "        'sql_db_query_checker',\n",
    "        'sql_db_query', \n",
    "        'check_result'\n",
    "    ]\n",
    "    if all(elem in it for elem in expected):\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "    return {\"score\": int(score), \"key\": \"multi_tool_call_in_order\"}\n",
    "\n",
    "def contains_all_tool_calls_in_order_exact_match(\n",
    "    root_run: Run, example: Example\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Check if all expected tools are called in exact order and without any \n",
    "        additional tool calls.\n",
    "    \"\"\"\n",
    "    expected = [\n",
    "        'sql_db_list_tables',\n",
    "        'sql_db_schema',\n",
    "        'sql_db_query_checker',\n",
    "        'sql_db_query',\n",
    "        'check_result'\n",
    "    ]\n",
    "    messages = root_run.outputs[\"response\"]\n",
    "    tool_calls = find_tool_calls(messages)\n",
    "    # Optionally, log the tool calls -\n",
    "    #print(\"Here are my tool calls:\")\n",
    "    #print(tool_calls)\n",
    "    if tool_calls == expected:\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "\n",
    "    return {\"score\": int(score), \"key\": \"multi_tool_call_in_exact_order\"}\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_sql_agent_messages,\n",
    "    data=dataset_name,\n",
    "    evaluators=[\n",
    "        contains_all_tool_calls_any_order,\n",
    "        contains_all_tool_calls_in_order,\n",
    "        contains_all_tool_calls_in_order_exact_match\n",
    "    ],\n",
    "    num_repetitions=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef1f7e",
   "metadata": {},
   "source": [
    "**Implementation Breakdown**\n",
    "\n",
    "This example includes several steps to evaluate an agent's trajectory:\n",
    "\n",
    "1. **Invocation of the Agent**  \n",
    "   - Uses `graph.invoke` to run a precompiled LangGraph agent with a specific prompt.\n",
    "\n",
    "2. **Specialized Agent Configuration**  \n",
    "   - Tools are embedded within the agent’s logic instead of being dynamically passed with the dataset input.\n",
    "\n",
    "3. **Tool Call Extraction**  \n",
    "   - Uses the function `find_tool_calls` to obtain the list of tools the agent used during execution.\n",
    "\n",
    "4. **Tool Call Verification**  \n",
    "   - Verifies whether:\n",
    "     - All expected tools were called, in any order: `contains_all_tool_calls_any_order`\n",
    "     - All expected tools were called in the correct order: `contains_all_tool_calls_in_order`\n",
    "     - All expected tools were called in the exact same order without variation: `contains_all_tool_calls_in_order_exact_match`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
