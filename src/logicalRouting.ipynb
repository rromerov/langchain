{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logical Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"List with all available datasources, the model chose from in order to generate the response\"\"\"\n",
    "\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"csharp_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"\"\"Given the user query, choose which of these datasources python_docs, js_docs, etc is the most useful to answer the question\"\"\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert at identifying which programming language is being given by the user\n",
    "\n",
    "Programming languages include python, js, among other, determine which programming language suits the best, your response should provide the \n",
    "datasource, datasources are the ones provided at the context.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we invoke the LLM to extract the data source based on the predefined schema:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'js_docs'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"console.log(\"Hello!\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})\n",
    "\n",
    "result.datasource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it's important to note that prompt engineering is a crucial step in this process. The quality of the prompt can significantly influence the model's performance and the relevance of the generated output. Once we've extracted the relevant data source, we can pass the value into another function to exectue additional logic as required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    available_routes ={\n",
    "        'python_docs':'chain for python_docs',\n",
    "        'js_docs':'chain for js_docs',\n",
    "        'csharp_docs':'chain for csharp_docs'\n",
    "    }\n",
    "    for key, value in available_routes.items():\n",
    "        if key == result.datasource.lower():\n",
    "            return value\n",
    "        \n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the result is lowercased to ensure consistency, in order to do our chain more resilient to the LLM going off script and producing output that doesn't quite conform to the schema we asked for, the next advanced routing technique is **Semantic Routing**. Let's go back to the main [README_file](../README.md/#Semantic_Routing)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
