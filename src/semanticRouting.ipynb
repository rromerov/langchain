{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A great question to start with! A black hole is an incredibly dense region of space where gravity is so strong that nothing, including light, can escape once it gets too close. It's formed when a massive star collapses in on itself, causing its gravitational pull to become impossibly strong.\n",
      "\n",
      "Think of it like this: imagine a vacuum cleaner that sucks up everything around it, but instead of dust and dirt, it sucks up space and time themselves. The point of no return, where anything that crosses the event horizon (the boundary around the black hole) gets trapped forever, is called the singularity. It's a one-way portal to the unknown.\n",
      "\n",
      "Black holes come in different sizes, ranging from small ones formed by the collapse of individual stars, to supermassive ones at the centers of galaxies, with masses millions or even billions of times that of our sun!\n"
     ]
    }
   ],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "# Two prompts\n",
    "physics_template = \"\"\"You are a very smart physics professor. You are great at \n",
    "    answering questions about physics in a concise and easy-to-understand manner. \n",
    "    When you don't know the answer to a question, you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering \n",
    "    math questions. You are so good because you are able to break down hard \n",
    "    problems into their component parts, answer the component parts, and then \n",
    "    put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "# Embed prompts\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "# Route question to prompt\n",
    "@chain\n",
    "def prompt_router(query):\n",
    "    # Embed question\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    # Compute similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    # Pick the prompt most similar to the input question\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "semantic_router = (\n",
    "    prompt_router\n",
    "    | ChatOllama(model=\"llama3.1\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(semantic_router.invoke(\"What's a black hole\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the next method, called **Query construction**. Click [here](../README.md/#query-construction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
