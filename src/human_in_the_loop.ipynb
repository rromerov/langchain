{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6676ea0",
   "metadata": {},
   "source": [
    "# Human in the loop\n",
    "\n",
    "---\n",
    "\n",
    "To enable any human-in-the-loop capabilities, the **first step** is to  \n",
    "**attach a checkpointer** to the graph.\n",
    "\n",
    "---\n",
    "\n",
    "With checkpointing in place, the application can pause, resume, inspect,  \n",
    "or fork based on human decisions, enabling interactive and controlled execution of LLM workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1b4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.vectorstores.in_memory import InMemoryVectorStore\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "@tool\n",
    "def calculator(query: str) -> str:\n",
    "    \"\"\"A simple calculator tool. Input should be a mathematical expression.\"\"\"\n",
    "    return ast.literal_eval(query)\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "model = AzureChatOpenAI(model=\"gpt-4o\", temperature=0.1, azure_deployment=\"gpt-4o\", api_version=\"2024-10-21\") \n",
    "\n",
    "tools_retriever = InMemoryVectorStore.from_documents(\n",
    "    [Document(tool.description, metadata={\"name\": tool.name}) for tool in tools],\n",
    "    embeddings,\n",
    ").as_retriever()\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    selected_tools: list[str]\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    selected_tools = [\n",
    "        tool for tool in tools if tool.name in state[\"selected_tools\"]\n",
    "    ]\n",
    "    res = model.bind_tools(selected_tools).invoke(state[\"messages\"])\n",
    "    return {\"messages\": res}\n",
    "\n",
    "def select_tools(state: State) -> State:\n",
    "    query = state[\"messages\"][-1].content\n",
    "    tool_docs = tools_retriever.invoke(query)\n",
    "    return {\"selected_tools\": [doc.metadata[\"name\"] for doc in tool_docs]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"select_tools\", select_tools)\n",
    "builder.add_node(\"model\", model_node)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "builder.add_edge(START, \"select_tools\")\n",
    "builder.add_edge(\"select_tools\", \"model\")\n",
    "builder.add_conditional_edges(\"model\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"model\")\n",
    "\n",
    "graph = builder.compile(checkpointer=MemorySaver()) # Add checkpointer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c930ac",
   "metadata": {},
   "source": [
    "When a checkpointer is attached, the graph returns an instance that:\n",
    "\n",
    "- **Saves the state** at the end of each step\n",
    "- Ensures that **subsequent invocations** do not start from scratch\n",
    "\n",
    "Each time the graph is called:\n",
    "\n",
    "1. It first uses the **checkpointer** to load the most recent saved state (if available).\n",
    "2. It then **merges the new input** with this saved state.\n",
    "3. Only after that does it begin executing the first nodes.\n",
    "\n",
    "This mechanism is essential for enabling **human-in-the-loop (HITL)** functionality,  \n",
    "as it allows the system to \"remember\" and build upon prior steps.\n",
    "\n",
    "---\n",
    "\n",
    "**Interrupt Mode**\n",
    "\n",
    "The simplest form of human control is the **interrupt** mechanism.  \n",
    "In this mode, the user:\n",
    "\n",
    "- Watches the **streaming output** in real time\n",
    "- **Manually interrupts** execution when needed (see Figure 8-3)\n",
    "\n",
    "When interrupted:\n",
    "\n",
    "- The graph’s **state is saved** as of the last **fully completed step**\n",
    "- The user is then presented with options:\n",
    "\n",
    "---\n",
    "\n",
    "1. **Resume**  \n",
    "   Continue execution from the point of interruption.  \n",
    "   The graph proceeds as if it had never been paused.\n",
    "\n",
    "2. **Restart**  \n",
    "   Provide **new input** (e.g., a new message in a chatbot).  \n",
    "   This **cancels any pending steps** and initiates a new computation from the updated input.\n",
    "\n",
    "3. **Do Nothing**  \n",
    "   Simply leave the execution as-is.  \n",
    "   No further steps will be executed unless triggered again.\n",
    "\n",
    "---\n",
    "\n",
    "This interrupt-resume-restart flow provides developers and users  \n",
    "with fine-grained control over complex LLM workflows,  \n",
    "helping to improve reliability, responsiveness, and user trust.\n",
    "\n",
    "Let’s see how to do this in LangGraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a545a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from contextlib import aclosing # Return an async context manager that calls the aclose() method of thing upon completion of the block.\n",
    "\n",
    "event = asyncio.Event()\n",
    "\n",
    "input = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\"\"\"How old was the 30th president of the United States \n",
    "            when he died?\"\"\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "async with aclosing(graph.astream(input, config)) as stream:\n",
    "    async for chunk in stream:\n",
    "        if event.is_set():\n",
    "            break\n",
    "        else:\n",
    "            ... # do something with the output\n",
    "\n",
    "# Somewhere else in your application\n",
    "\n",
    "event.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef1e45",
   "metadata": {},
   "source": [
    "This functionality relies on using an **event or signal**  \n",
    "to allow **external control** over interruption—meaning you can halt execution  \n",
    "even from outside the application itself.\n",
    "\n",
    "---\n",
    "\n",
    "### Language-Specific Considerations\n",
    "\n",
    "**Python**\n",
    "\n",
    "- Notice the use of `aclosing` in the Python code example.\n",
    "- This ensures that the **async stream is properly closed** when interrupted.\n",
    "- Failing to close the stream correctly can result in **resource leaks** or incomplete state persistence.\n",
    "\n",
    "\n",
    "### Thread Identification with the Checkpointer\n",
    "\n",
    "When using a **checkpointer**, it is important to provide an **identifier for the current thread**.  \n",
    "This identifier is used to distinguish one interaction (or session) with the graph from all others.\n",
    "\n",
    "- This is critical when you are supporting **multiple concurrent users** or workflows.\n",
    "- It ensures each graph run accesses and resumes the **correct saved state**.\n",
    "\n",
    "---\n",
    "\n",
    "Together, these techniques support robust, externally-interruptible LLM workflows  \n",
    "with proper session handling and graceful stream termination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef6ec55",
   "metadata": {},
   "source": [
    "A second human-in-the-loop control mode is **authorize**.\n",
    "\n",
    "In this mode, the user specifies **ahead of time** that they want the application to **pause and request approval** whenever a specific node is about to run.\n",
    "\n",
    "\n",
    "This is typically used for **tool confirmation**, allowing users to:\n",
    "\n",
    "- Review tool calls before they are made\n",
    "- Prevent undesired or high-impact actions\n",
    "- Insert new guidance when needed\n",
    "\n",
    "User Options on Pause\n",
    "\n",
    "When the application pauses before executing the node, the user can:\n",
    "\n",
    "1. **Resume**  \n",
    "   Approve the tool call and continue computation as planned.\n",
    "\n",
    "2. **Redirect**  \n",
    "   Provide **new input or instructions** to steer the conversation in a different direction.  \n",
    "   The tool will **not** be called.\n",
    "\n",
    "3. **Do Nothing**  \n",
    "   The application remains paused until the user acts.\n",
    "\n",
    "This mode enhances **trust and control** in higher-agency applications,  \n",
    "giving users the final say before potentially irreversible or expensive actions.\n",
    "\n",
    "Here’s the code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18ebb5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\"\"\"How old was the 30th president of the United States when he died?\"\"\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "output = graph.astream(input, config, interrupt_before=[\"tools\"])\n",
    "\n",
    "async for c in output:\n",
    "    ... # do something with the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4ddf1",
   "metadata": {},
   "source": [
    "When using the **authorize** control mode, you can configure the graph to:\n",
    "\n",
    "- **Pause execution** right before entering a specific node (e.g., `tools`)\n",
    "- Allow for **manual inspection and decision-making** based on the current state\n",
    "\n",
    "This gives the user a moment to review what’s about to happen,  \n",
    "and either approve or redirect the flow.\n",
    "\n",
    "---\n",
    "\n",
    "### interrupt_before\n",
    "\n",
    "- This is a **list of node names** where execution should pause.\n",
    "- The **order of the list does not matter**—interruption will occur before each listed node.\n",
    "- Example: `interrupt_before=[\"tools\"]`  \n",
    "  This causes the graph to pause **just before** entering the `tools` node.\n",
    "\n",
    "This setup is useful for use cases such as tool call approval, safety validation, or manual intervention checkpoints.\n",
    "\n",
    "---\n",
    "\n",
    "### Resume\n",
    "\n",
    "To **resume** from an interrupted state—whether due to an `interrupt` or `authorize` mode—  \n",
    "you simply **re-invoke the graph** with:\n",
    "\n",
    "- `null` (JavaScript)\n",
    "- `None` (Python)\n",
    "\n",
    "This signals the graph to continue processing the **last valid input**,  \n",
    "rather than starting a new interaction.\n",
    "\n",
    "No new user input is required—the graph picks up exactly where it left off,  \n",
    "based on the **last saved state** in the checkpointer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de9b8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "output = graph.astream(None, config, interrupt_before=[\"tools\"])\n",
    "\n",
    "async for c in output:\n",
    "    ... # do something with the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd287e8",
   "metadata": {},
   "source": [
    "**Restart**\n",
    "\n",
    "If, instead of resuming, you want the **interrupted graph to start over**  \n",
    "from the beginning—using new input—you can do so easily.\n",
    "\n",
    "To **restart** execution:\n",
    "\n",
    "- Simply **invoke the graph with new input**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b604c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\"\"\"How old was the 30th president of the United States \n",
    "            when he died?\"\"\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "output = graph.astream(input, config)\n",
    "\n",
    "async for c in output:\n",
    "    ... # do something with the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf8918e",
   "metadata": {},
   "source": [
    "When restarting a graph with new input:\n",
    "\n",
    "- The system will **retain the current state**\n",
    "- It will **merge that state with the new input**\n",
    "- Execution will then **restart from the first node**\n",
    "\n",
    "---\n",
    "\n",
    "If your goal is to **discard the current state entirely** and start from scratch:\n",
    "\n",
    "- Simply **change the `thread_id`**\n",
    "- This triggers a **new interaction**, initialized from a **blank slate**\n",
    "\n",
    "> Any string value is a valid `thread_id`  \n",
    "> For best practice, use **UUIDs** or other unique identifiers  \n",
    "> to ensure thread sessions are clearly separated\n",
    "\n",
    "This approach gives you control over session scoping—  \n",
    "letting you restart cleanly when needed, while still leveraging LangGraph’s state persistence when desired.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279670c5",
   "metadata": {},
   "source": [
    "**Edit State**\n",
    "\n",
    "There may be situations where you want to **manually update the graph’s state**  \n",
    "before resuming execution—this is fully supported.\n",
    "\n",
    "LangGraph provides two key methods for this:\n",
    "\n",
    "- `get_state`: Allows you to **inspect the current state** of the graph.\n",
    "- `update_state`: Lets you **modify the state** before continuing.\n",
    "\n",
    "This is useful for:\n",
    "\n",
    "- Correcting values\n",
    "- Injecting new context\n",
    "- Overriding intermediate outputs\n",
    "\n",
    "---\n",
    "\n",
    "Here’s what the typical workflow looks like:\n",
    "\n",
    "1. Use `get_state(thread_id)` to **fetch the current saved state**\n",
    "2. Make desired changes to the state dictionary\n",
    "3. Use `update_state(thread_id, updated_state)` to **persist your changes**\n",
    "4. Resume or restart the graph as needed\n",
    "\n",
    "Let’s see what this looks like in code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "210a890f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f03c0c3-5b5d-626c-800b-632be1932e69'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "state = graph.get_state(config)\n",
    "\n",
    "# somwthing you want to add or replace\n",
    "update = {}\n",
    "\n",
    "graph.update_state(config, update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457754cf",
   "metadata": {},
   "source": [
    "Once you call `update_state`, a **new checkpoint** is created that includes your modifications.  \n",
    "From this point forward, you can **resume the graph** using this updated state.\n",
    "\n",
    "> See the “Resume” section for details on how to continue execution after editing state.\n",
    "\n",
    "---\n",
    "\n",
    "**Fork**\n",
    "\n",
    "Another powerful capability is the ability to **browse the history of all past states**  \n",
    "that the graph has passed through.\n",
    "\n",
    "From this history, you can:\n",
    "\n",
    "- **Select any prior state**\n",
    "- **Resume execution from that point**\n",
    "- Explore **alternate outputs** or responses\n",
    "\n",
    "This is especially useful in **creative applications**, where:\n",
    "\n",
    "- Each run through the graph may result in different behavior or outputs\n",
    "- You want to compare multiple potential outcomes from the same starting input\n",
    "\n",
    "This feature allows for experimentation, decision branching, and creative exploration.\n",
    "\n",
    "Let’s take a look at how to implement this in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0858b3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='How old was the 30th president of the United States \\n            when he died?', additional_kwargs={}, response_metadata={}, id='51903e9e-53be-4974-bd75-d348d85ee612'),\n",
       "  AIMessage(content='The 30th president of the United States was Calvin Coolidge. He was born on July 4, 1872, and died on January 5, 1933. To calculate his age at the time of his death:\\n\\n1933 - 1872 = 61 years old.\\n\\nCalvin Coolidge was 61 years old when he died.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 116, 'total_tokens': 191, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-BcFU4sMaeYXsc3kkDdwQdSLTIuJGf', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run--02e2d8fd-2e55-4446-8cbd-06a2fa5b4abe-0', usage_metadata={'input_tokens': 116, 'output_tokens': 75, 'total_tokens': 191, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='How old was the 30th president of the United States when he died?', additional_kwargs={}, response_metadata={}, id='d6469a60-2c33-40f2-a24a-89eda594cabb'),\n",
       "  AIMessage(content='The 30th president of the United States, Calvin Coolidge, was born on **July 4, 1872**, and died on **January 5, 1933**. To calculate his age at the time of his death:\\n\\n1. From **July 4, 1872** to **July 4, 1932**, he turned 60 years old.\\n2. From **July 4, 1932** to **January 5, 1933**, he lived an additional 6 months.\\n\\nThus, Calvin Coolidge was **60 years old** when he died.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 126, 'prompt_tokens': 213, 'total_tokens': 339, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-BcINSFgTt95EHhD4dVcagJy6RdXO0', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run--1caa1d7d-44a4-49ec-a9a4-11fd9f53f5b0-0', usage_metadata={'input_tokens': 213, 'output_tokens': 126, 'total_tokens': 339, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='How old was the 30th president of the United States \\n            when he died?', additional_kwargs={}, response_metadata={}, id='e7bc2601-0b57-4b07-a6e3-518f71d1c369'),\n",
       "  AIMessage(content='The 30th president of the United States, Calvin Coolidge, was born on **July 4, 1872**, and died on **January 5, 1933**. To calculate his age:\\n\\n- From **July 4, 1872**, to **July 4, 1932**, he turned **60 years old**.\\n- From **July 4, 1932**, to **January 5, 1933**, he lived an additional **6 months**.\\n\\nThus, Calvin Coolidge was **60 years old** when he passed away.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 363, 'total_tokens': 484, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-BcJ24MDdnlREHiR5J2oVKmetcMp1t', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run--8862447a-1ff3-4b3d-b56b-0404d2bae907-0', usage_metadata={'input_tokens': 363, 'output_tokens': 121, 'total_tokens': 484, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'selected_tools': ['duckduckgo_search', 'calculator']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "history = [\n",
    "    state for state in\n",
    "    graph.get_state_history(config)\n",
    "]\n",
    "\n",
    "# replay a past state\n",
    "graph.invoke(None, history[2].config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4806e75c",
   "metadata": {},
   "source": [
    "Notice how we **collect the state history** into a `list` (Python) or `array` (JavaScript).  \n",
    "The `get_state_history` method returns an **iterator** of states to allow for **lazy consumption**.\n",
    "\n",
    "- The returned states are **sorted** from **most recent to oldest**\n",
    "- This makes it easy to access the **latest checkpoints first**, which is often the most relevant for resuming or forking\n",
    "\n",
    "---\n",
    "\n",
    "The **real strength** of LangGraph's human-in-the-loop capabilities  \n",
    "comes from your ability to **combine control modes** in a way that best suits your application.\n",
    "\n",
    "For example, you can:\n",
    "\n",
    "- Interrupt execution at key nodes\n",
    "- Require authorization before tool use\n",
    "- Edit or fork state between steps\n",
    "- Stream intermediate output for live inspection\n",
    "\n",
    "This **flexible, composable design** allows developers to build highly responsive,  \n",
    "interactive, and user-centric LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468cd95",
   "metadata": {},
   "source": [
    "Notice how we collect the **state history** into a `list` or `array` depending on the language used.  \n",
    "The `get_state_history` method returns an **iterator**, which supports **lazy consumption**.\n",
    "\n",
    "- The states are sorted from **most recent to oldest**.\n",
    "- This makes it easy to retrieve the most relevant checkpoints first.\n",
    "\n",
    "---\n",
    "\n",
    "The **true power** of human-in-the-loop controls lies in the ability to **mix them**  \n",
    "in whatever way best suits your application.\n",
    "\n",
    "---\n",
    "\n",
    "**Multitasking LLMs**\n",
    "\n",
    "This section covers how to handle **concurrent input** in LLM applications.  \n",
    "This is an increasingly relevant issue due to:\n",
    "\n",
    "- The relatively **high latency** of LLMs, especially for long responses or chained steps.\n",
    "- The natural evolution of applications toward **more complex use cases** as LLM speed improves.\n",
    "\n",
    "Even fast models will encounter challenges with concurrency as demand scales—  \n",
    "just like humans need to prioritize competing tasks.\n",
    "\n",
    "Let’s walk through the available strategies.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Refuse Concurrent Inputs**\n",
    "\n",
    "- Any input received while another is being processed is **rejected**.\n",
    "- This is the **simplest approach**, but not ideal for most applications.\n",
    "- It **offloads concurrency handling** to the caller (user or external system).\n",
    "\n",
    "---\n",
    "\n",
    "**2. Handle Independently**\n",
    "\n",
    "- Treat each new input as an **independent invocation**.\n",
    "- Creates a new thread (state container) and processes it in parallel.\n",
    "- Pros:\n",
    "  - **Scales well** to many users or contexts.\n",
    "- Cons:\n",
    "  - May appear as **disconnected interactions** to the user.\n",
    "  - Not ideal when continuity or context sharing is required.\n",
    "\n",
    "Use case: Running multiple chat sessions with different users concurrently.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Queue Concurrent Inputs**\n",
    "\n",
    "- New inputs are **queued** and handled in order, one after another.\n",
    "- Pros:\n",
    "  - Supports an **unlimited number** of concurrent requests.\n",
    "  - Simple to implement and reason about.\n",
    "- Cons:\n",
    "  - Input may become **stale** by the time it’s processed.\n",
    "  - The **queue can grow indefinitely** if inputs arrive faster than they are processed.\n",
    "  - Not suitable when new input depends on the previous output.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Interrupt**\n",
    "\n",
    "- On receiving new input, **abort current execution** and begin processing the new input.\n",
    "- Variants:\n",
    "  - **Keep nothing**: Completely discard the previous input and its progress.\n",
    "  - **Keep the last completed step**: Retain state up to the last successful node.\n",
    "  - **Keep the last completed and current step**: Try to preserve any partial updates in progress.\n",
    "  - **Wait for current node to finish**: Resume only after the active node completes.\n",
    "\n",
    "- Pros:\n",
    "  - **Faster response** to new input.\n",
    "  - Reduces risk of stale outputs.\n",
    "- Cons:\n",
    "  - Only one input is processed at a time.\n",
    "  - Can leave the graph in an **inconsistent state** if not properly handled.\n",
    "  - Output can be **brittle or unpredictable**, depending on when the interruption occurs.\n",
    "\n",
    "Example: Interrupting a tool call before completion can cause invalid state in some LLMs like OpenAI Chat.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Fork and Merge**\n",
    "\n",
    "- On new input, **fork the current state** and process the new input in parallel.\n",
    "- Merge final states after each branch completes.\n",
    "\n",
    "- Requirements:\n",
    "  - State must be **mergeable**, either:\n",
    "    - Automatically (e.g., via CRDTs)\n",
    "    - Or with **manual conflict resolution** by the user.\n",
    "\n",
    "- Pros:\n",
    "  - Handles **new input promptly**.\n",
    "  - Output is **independent of timing**.\n",
    "  - Supports **arbitrary concurrency**.\n",
    "- Cons:\n",
    "  - Requires **designing the state system carefully** to handle merges safely.\n",
    "\n",
    "This is the **most flexible and powerful strategy** if your app supports conflict resolution.\n",
    "\n",
    "---\n",
    "\n",
    "Each of these strategies represents a **design choice** about how to balance:\n",
    "\n",
    "- Responsiveness\n",
    "- Consistency\n",
    "- Complexity\n",
    "- User experience\n",
    "\n",
    "Choose the one that best fits your application’s architecture and user expectations.\n",
    "\n",
    "Let's go back to the main [file](../README.md#what-is-langchain)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
