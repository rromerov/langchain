{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "427e2171",
   "metadata": {},
   "source": [
    "Using the connection string to Azure Application Insights directly:\n",
    "\n",
    "Go to Azure AI Foundry portal and select Tracing.\n",
    "\n",
    "Select Manage data source. In this screen you can see the instance that is associated with the project.\n",
    "\n",
    "Copy the value at Connection string and set it to the following variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecbe10f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46cbb24e",
   "metadata": {},
   "source": [
    "```bash\n",
    "export CONNECTION_STRING=\"Instrumentation...\"\n",
    "```\n",
    "\n",
    "### Enable tracing for Langchain\n",
    "You can enable tracing for Langchain that follows OpenTelemetry standards as per [opentelemetry-instrumentation-langchain](https://pypi.org/project/opentelemetry-instrumentation-langchain/). To enable tracing for Langchain, install the package `opentelemetry-instrumentation-langchain` using your package manager, like pip:\n",
    "\n",
    "```bash\n",
    "pip install opentelemetry-instrumentation-langchain\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_azure_ai.callbacks.tracers.inference_tracing import AzureAIInferenceTracer\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_azure_ai.callbacks.tracers import AzureAIInferenceTracer\n",
    "\n",
    "# os.environ[\"AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED\"] = \"true\" # False by default\n",
    "\n",
    "# Full langchain traces\n",
    "# read more here https://pypi.org/project/opentelemetry-instrumentation-langchain/\n",
    "\n",
    "# Get the resource name from environment variables\n",
    "\n",
    "model = AzureChatOpenAI(model=\"gpt-4o\", azure_deployment=\"gpt-4o\", temperature=0.0, api_version=\"2024-10-21\")\n",
    "\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "# get the connection string from environment variables\n",
    "application_insights_connection_string = os.environ[\"CONNECTION_STRING\"]\n",
    "# create the tracer with the connection string and enable content recording\n",
    "tracer = AzureAIInferenceTracer(connection_string=application_insights_connection_string,\n",
    "                                enable_content_recording=True,\n",
    ")\n",
    "\n",
    "# add the tracer to the LLM\n",
    "chain = chain.with_config({\"callbacks\": [tracer]})\n",
    "chain.invoke({\"language\": \"spanish\", \"text\": \"hi\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d2b0ea",
   "metadata": {},
   "source": [
    "Using LangChain OpenAI the following can be seen:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed323ee",
   "metadata": {},
   "source": [
    "![Langchain Traces in Azure AI Foundry portal](../img/ai_foundry_traces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b1364b",
   "metadata": {},
   "source": [
    "Let's go back to the [main file](../README.md#Production)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
