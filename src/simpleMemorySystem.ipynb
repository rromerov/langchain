{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Memory System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I translated \"I love programming\" from English to French, and the translation is \"J\\'adore programmer\".', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-03-31T18:55:39.2506507Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10182195600, 'load_duration': 4919371400, 'prompt_eval_count': 65, 'prompt_eval_duration': 1661000000, 'eval_count': 24, 'eval_duration': 3064000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-c0c688ff-ca69-475f-9ef4-38854d4660ef-0', usage_metadata={'input_tokens': 65, 'output_tokens': 24, 'total_tokens': 89})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"\"\"You are a helpful assistant. Answer all questions to the best of your ability.\"\"\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "model = ChatOllama(model= \"llama3.1\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\n",
    "    \"messages\":[\n",
    "        (\"human\",\"\"\"Translate this sentence from English to French: I love programming.\"\"\"),\n",
    "        (\"ai\",\"J'adore programmer.\"),\n",
    "        (\"human\",\"What did you just say?\"),\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the incorporation of the previous conversation in the chain enabled the model to answer the follow-up question in a context-aware manner. While this is simple and works, when taking your application to production, more challenges will show related to managing memory at scale, such as:\n",
    "\n",
    "- You will need to update the memory after every interaction, atomically (i.e., don't record only the question or only the answer in the case of failure).\n",
    "- You want to store these memories in durable storage, such as relational database.\n",
    "- You want to control how many and which messages are stored for later, and how many of these are used for new interactions.\n",
    "- You will want to inspect and modify this state (for now, just a list of messages) outside a call to an LLM.\n",
    "\n",
    "Let's go back to the [main file](../README.md/#LangGraph)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
